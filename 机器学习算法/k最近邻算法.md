K 最近邻（K - Nearest Neighbor，KNN）算法是一种基本的分类与回归算法。
# 算法原理
KNN 算法的核心思想是基于数据的相似性进行分类或预测。对于一个新的待分类样本，算法会在训练数据集中找到与它距离最近的 K 个邻居，然后根据这 K 个邻居的类别或属性来确定待分类样本的类别或预测值。

通常使用距离度量来衡量样本之间的相似性，如欧氏距离、曼哈顿距离等。
# 算法步骤
- 数据准备

    收集并整理带有类别标签（分类问题）或数值标签（回归问题）的训练数据集。
- 计算距离

    对于待分类的新样本，计算它与训练数据集中每个样本的距离。
- 选择 K 个最近邻

    根据计算出的距离，选取距离新样本最近的 K 个训练样本。
- 分类或预测

    分类任务：统计这 K 个最近邻中各类别出现的频率，将新样本归为出现频率最高的类别。

    回归任务：计算这 K 个最近邻的数值标签的平均值或加权平均值，将其作为新样本的预测值。权重通常与距离成反比，即距离越近的邻居权重越高。
# 算法优点
简单直观：算法原理易于理解和实现，不需要复杂的模型训练过程。

适应性强：对数据的分布没有严格要求，能处理各种类型的数据，包括数值型、类别型等。

准确性较高：在数据量较大且数据分布较为均匀的情况下，KNN 算法通常能取得较好的分类和预测效果。
# 算法缺点
计算量大：在分类或预测时，需要计算待分类样本与所有训练样本的距离，当训练数据集较大时，计算成本较高，导致算法效率较低。

内存占用高：需要存储整个训练数据集，对于大规模数据集，可能会占用大量的内存空间。

对 K 值敏感：K 值的选择对算法的性能影响较大。K 值过小，容易受到噪声数据的影响，导致模型过拟合；K 值过大，可能会使分类或预测结果过于模糊，忽略了数据的局部特征。
# 应用场景
图像识别：将待识别图像的特征与已标注的图像特征进行对比，通过 KNN 算法找出最相似的 K 个图像，从而确定待识别图像的类别，如识别手写数字、区分不同类型的物体等。

医疗诊断：根据患者的症状、检查结果等特征数据，与已有的病例数据进行对比，利用 KNN 算法找出相似的病例，辅助医生进行疾病诊断和治疗方案制定。

客户细分：根据客户的各种属性特征，如年龄、收入、消费行为等，使用 KNN 算法将新客户划分到不同的客户群体中，以便企业进行精准营销和个性化服务。
