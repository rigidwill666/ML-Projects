在Transformer模型中，输入数据是源语言的句子（如英文），标签是目标语言的句子（如法文）。
# 总框架
- Encoder：把输入序列（如英文句子）转化为隐藏向量（理解输入）。
- Decoder：根据隐藏向量生成输出序列（如法文句子）。
- 中间靠注意力（Attention）机制连接。
## Encoder结构
每个Encoder层（会堆叠多个）包含两个子模块：

1. 多头自注意力机制

    输入内部的各位置相互关注。比如：句子里第一个词，可以看第十个词来决定自身表示。

2. 前馈全连接网络（FFN）

    每个位置独立地通过一个小型神经网络处理。

每个子模块后还有：

- 残差连接（Residual Connection）：跳跃连接，防止梯度消失。
- Layer Normalization（层归一化）：加速训练稳定性。

> 总结：【输入】→【多头注意力】→【残差+归一化】→【前馈网络】→【残差+归一化】→【输出】
## Decoder结构
每个Decoder层包含三个子模块：

1. Masked 多头自注意力机制

    防止看到未来的词（预测第3个词时只能看前2个词）。

2. Encoder-Decoder Attention

    当前Decoder的词能去关注整个Encoder输出（比如，法语词可以关注原英文哪些词）。

3. 前馈全连接网络（FFN）

同样，每个子模块后跟：

- 残差连接
- LayerNorm

> 总结：【输入】→【Masked注意力】→【残差+归一化】→【跨注意力】→【残差+归一化】→【前馈网络】→【残差+归一化】→【输出】
## Attention
自注意力打分公式：

$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

自注意力中除以 $\sqrt{d_k}$，是为了防止点积结果随着维度变大导致softmax过尖，保持训练稳定和高效。
# 分类
## Encoder-only 模型
代表：BERT

特点：只用Encoder部分，双向（上下文）建模；适合**理解类**任务：分类、命名实体识别、问答、推理。
| 变体      | 特点 |
| :---        |    :----:   |
| DistilBERT	| 把BERT变小（压缩为一半大小），速度快，性能损失小 |
| RoBERTa	| 移除了NSP，只用MLM，大量增加了训练量，效果更好 |
| ALBERT	| 用参数共享+矩阵分解，极大减少参数量（加速、节省内存）|
| SpanBERT	| 改进MLM任务：遮盖连续片段而不是单词，提高句子级理解 |
| TinyBERT	| 更小版本，专门用于移动设备/边缘设备部署 |
## Decoder-only 模型
代表：GPT系列

特点：只用Decoder部分，单向（通常是左到右）建模；适合生成类任务：文本生成、对话、编程。
| 变体      | 特点 |
| :---        |    :----:   |
| GPT-1	|	首次提出，117M参数，小规模，验证可行性 |
| GPT-2	| 1.5B参数，强大生成能力（因太强一度拒绝开源） |
| GPT-3	| 175B参数，极大规模，少量样本学习（few-shot）成为现实，目前没有开源 |
| GPT-4 | 多模态（图像+文本），推理和稳健性大幅增强 |
## Encoder-Decoder模型
代表：T5（Text-to-Text Transfer Transformer）、BART。

特点：输入和输出都当作文本处理；适合复杂转换类任务：摘要、翻译、对话生成等。
