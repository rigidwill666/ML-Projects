# 流程概览
## 数据收集
数据来源包括：公共数据集（如Common Crawl、Wikipedia、BooksCorpus）、编程平台（如GitHub代码）、社交媒体（如Reddit、Twitter，需严格合规）、新闻文章、博客、问答社区（如Quora、StackOverflow）、专业文本（如医学、法律文献）
## 数据清洗
清洗是去除无用、低质量或错误信息的关键步骤：移除乱码/广告/HTML标签等噪声内容、滤除非目标语言的内容（如非英文非中文内容）、去除重复句段/无意义文本/低质量网页、对敏感词/攻击性语言等进行筛查与过滤
## 数据标注
预训练阶段一般是自监督学习，不需要人工标注。但在某些场景中：可加入一部分“监督样本”（如问答对话、摘要数据）、多任务预训练可能需要标注（如NLI、情感分类）
## 数据去重与脱敏
保障数据安全性与隐私性的重要流程：
- 去重：减少模型对重复样本的“过拟合”
- 脱敏：删除邮箱/手机号/地址等隐私字段、替换人名或使用差分隐私等技术处理、使用规则或模型进行敏感信息识别与替换
## 格式化与切分
将文本处理为模型可接受的格式：
- 分词（中文可使用BPE、WordPiece、SentencePiece等）
- 构建输入序列、加入特殊token（如[CLS]、[SEP]）
- 控制最大长度，截断或填充
- 生成训练目标：GPT：下一个词预测，BERT：掩码语言建模
## 训练数据构建
按比例划分训练集、验证集

构建多语言或多模态数据结构（图像、音频可扩展）

数据打包为二进制格式（如TFRecord、WebDataset等）加速读取

分布式系统进行异步加载（例如与PyTorch DataLoader结合）
## 持续数据更新
随时间引入新数据（保持对当前世界的认知）

解决“知识过时”问题

微调（fine-tuning）或后训练（post-training）更新模型
