在Transformer模型中，位置编码（Positional Encoding）是至关重要的部分，因为模型本身并没有像RNN或CNN那样的顺序处理能力。为了让模型理解输入词的顺序，Transformer引入了位置编码。

位置编码分为绝对位置编码和相对位置编码。这两者的核心差异在于它们如何表示位置关系。
# 分类
## 绝对位置编码
绝对位置编码为每个词的嵌入添加一个固定的位置信息向量。这些向量是通过位置编码函数计算的，通常是根据词的位置生成的一个**固定的、唯一的**向量。在Transformer的原始设计中使用了正弦和余弦函数来生成这种位置编码。

计算方式：

$$
PE(i,2j)=sin(\frac{i}{10000^{\frac{2j}{d}}})
$$
$$
PE(i,2j+1)=cos(\frac{i}{10000^{\frac{2j}{d}}})
$$

其中，𝑖 是词的位置索引，𝑗 是位置编码的维度，𝑑 是嵌入的维度

主要特点：
- 固定且不变：绝对位置编码是根据词的绝对位置来生成的，所以每个位置的编码在整个训练过程中是固定的，不会改变。
- 不考虑上下文：它依赖于词的绝对位置，因此无法捕捉到词与词之间的相对位置关系。

优点：
- 简单，易于实现，适用于大部分应用场景
  
缺点：
- 由于位置编码是固定的，它无法灵活地适应变动的输入顺序，尤其是在长文本或需要捕捉长距离依赖的任务中，表现较差。
## 相对位置编码
相对位置编码关注词之间的相对距离，而不是每个词的绝对位置。这种编码方法能够使模型理解词之间的距离或相对顺序，而不仅仅是它们在输入序列中的固定位置。相对位置编码是通过对位置差进行编码，从而使得模型能捕捉到输入序列中各个元素间的关系。

相对位置编码的实现方式有多种，其中最常见的一种方式是通过在**注意力机制中直接引入相对位置差异**。常见的相对位置编码方法有以下几种：

- 学习式相对位置编码：这类方法通过训练得到相对位置的权重，例如，采用基于学习的相对位置编码向量，每个相对距离 𝑟 都有一个对应的向量。
- 相对位置编码与注意力结合：例如，一些研究通过将相对位置编码直接加到**注意力权重矩阵**中，使得模型在计算注意力得分时，能够考虑词语之间的相对位置。

主要特点：
- 动态可调：相对位置编码是根据位置差异动态计算的，可以在不同的上下文中捕捉到更加灵活的信息。
- 能捕捉局部和长程依赖：相对位置编码更适合捕捉词语之间的相对位置关系，因此能在处理长文本或具有局部依赖的任务中，表现更好。

优点：
- 更加灵活，可以适应长序列。能够捕捉到长程依赖，因为它专注于词与词之间的相对关系，而不是固定的绝对位置。

缺点：
- 相对位置的计算复杂度较高，模型实现难度大
