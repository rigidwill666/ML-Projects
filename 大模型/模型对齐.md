模型对齐（Model Alignment）是大语言模型发展中的关键技术之一，其目标是让模型生成的内容符合人类的价值观、偏好与期望行为，而不仅仅是提升预测能力。
# 核心流程（RLHF范式）
1. 预训练（Pretraining）：使用大规模通用文本进行自监督训练（比如GPT）。
2. 监督微调（Supervised Fine-tuning，SFT）：用人工标注的问答对进行微调，使模型学会“看起来合理”的回应方式。
3. 偏好建模（Reward Modeling）：让人类对多个模型回复排序，然后训练一个奖励模型（Reward Model）。
4. 强化学习微调（如PPO、DPO等）：使用奖励模型对语言模型进行微调，引导其更符合人类偏好。
# 主流对齐算法
## PPO
> 最早被OpenAI用于训练InstructGPT、ChatGPT的核心算法。

PPO是一种强化学习算法，用于最大化“奖励函数”给出的反馈（如人类偏好）。
### 构造流程
- 构建奖励模型（Reward Model）：使用多个回答进行排序，训练一个模型打分函数 R(x,y)，用于衡量回答 y 对问题 x 的好坏。
- 使用当前语言模型生成答案。
- 用奖励模型评估答案好坏。
- 使用PPO算法微调语言模型，使其生成更高评分的答案。
### 特点
- 强化学习方法，优化过程不稳定，调参复杂。
- 有“策略更新的步幅限制”，防止模型“崩坏”。 
## DPO
> OpenAI 2023年提出的一种无需奖励模型的对齐方法，本质上是分类对比学习。

DPO直接使用人类偏好数据来进行对比学习，而非间接训练奖励模型。
### 构造流程
- 数据为 (x,y+,y−)：表示对于输入 x，人类偏好 y+ 多于 y−。
- 构造对比损失,类似于softmax分类，用模型的log概率直接构建偏好判断。
- 用这个损失函数来训练语言模型。
### 特点
- 直接使用偏好对，无需训练reward model。
- 无强化学习步骤，稳定、高效。
- 可解释性更强，适合大规模训练。
## GRPO
> 是一种结合生成-检索机制与偏好对齐的算法，由Anthropic等提出。

GRPO本质上是DPO的一种扩展，在生成模型与**检索模型（如RAG）** 之间做偏好建模。
### 构造流程
- 给定输入 x，模型生成两种回答 y+, y−（可能来自检索或不同方式生成）。
- 使用偏好学习方式（如DPO损失）训练模型，使其学会生成被人类偏好的回答。
- 同时训练生成模型和/或检索模块，提高最终输出质量。
### 特点
- 适用于多模态对齐或带外部信息的系统（如RAG）。
- 结合生成能力与检索机制，提高答案可控性。
- 对资源要求高，但效果稳定且更可泛化。
