# 流程
## 数据准备
数据来源：通用任务数据（如分类、问答、摘要等）、行业领域数据（如医疗、法律、金融等）、人工对话数据（如多轮问答、命令执行等）
## 数据预处理
分词（tokenization）：用和预训练时相同的分词器

截断与填充（truncation/padding）

构建输入格式：如添加特殊token ([CLS], [SEP]等)

标签构造：根据任务生成目标输出，如标签、响应文本
## 微调方式
### 全参数微调
调整全部模型参数，精度高但计算/存储开销大
### 参数高效微调
LoRA ，只调小块低秩矩阵，高效参数少但对底层结构略有限制

Prompt Tuning	，学习可调前缀token，极其轻量但效果依赖任务

Adapter	，插入微调模块，灵活但略增模型延迟
## 模型训练配置
损失函数：多为交叉熵（Cross Entropy Loss）

优化器：AdamW + 学习率调度器（warm-up + cosine）

训练参数：
  - batch size（通常小，为了显存）
  - learning rate（建议较低：1e-5～5e-6）
  - epoch（1～3一般足够）
## 评估与验证
使用验证集计算准确率、BLEU、ROUGE、F1、Perplexity等指标

人工评价生成质量是否符合任务预期（尤其是对话类任务）
## 模型保存与部署
使用 torch.save() 或 transformers.save_pretrained() 保存

可转换为 ONNX、ggml、TensorRT 等格式部署到服务端或边缘设备

在 API 服务中部署（FastAPI / vLLM / Triton）


